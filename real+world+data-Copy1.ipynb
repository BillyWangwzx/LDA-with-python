{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LDAsmooth import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma, polygamma\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('ap.txt', 'r')\n",
    "text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i',\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\n",
    "             \"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\n",
    "             \"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\n",
    "             \"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\n",
    "             \"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\n",
    "             \"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\n",
    "             \"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\n",
    "             \"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\n",
    "             \"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\n",
    "             \"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\n",
    "             \"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\n",
    "             \"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\n",
    "             \"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\n",
    "             \"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\n",
    "             \"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\n",
    "             \"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\n",
    "             \"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\n",
    "             \"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\n",
    "             \"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\n",
    "             \"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\n",
    "             \"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\n",
    "             \"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\n",
    "             \"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\n",
    "             \"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\n",
    "             \"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\n",
    "             \"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\n",
    "             \"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\n",
    "             \"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\n",
    "             \"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\n",
    "             \"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\n",
    "             \"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\n",
    "             \"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\n",
    "             \"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\n",
    "             \"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\n",
    "             \"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\n",
    "             \"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\n",
    "             \"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\n",
    "             \"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\n",
    "             \"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\n",
    "             \"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\n",
    "             \"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\n",
    "             \"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\n",
    "             \"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\n",
    "             \"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\n",
    "             \"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\n",
    "             \"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\n",
    "             \"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\n",
    "             \"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\n",
    "             \"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\n",
    "             \"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\n",
    "             \"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\n",
    "             \"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\n",
    "             \"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\n",
    "             \"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\",\n",
    "             \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "             \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\",\n",
    "             \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \n",
    "             \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
    "             \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in text:\n",
    "    if '<DOC>\\n' in t or '</DOC' in t or 'TEXT>' in t or '<DOC' in t or '<DOCNO>' in t:\n",
    "        text.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = list(map(lambda x: x.lower(),text))\n",
    "txt = list(map(lambda x: x.strip(),txt))\n",
    "txt = list(map(lambda x: re.sub('[0-9]+', '', x),txt))\n",
    "txt = list(map(lambda x: x.translate(str.maketrans('', '', string.punctuation)),txt))\n",
    "txt = list(map(lambda x: x.split(),txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tt in txt:\n",
    "    for i in tt:\n",
    "        if i in set(stopwords):\n",
    "            tt.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6750"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = txt[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countt = {id: Counter(doc) for id, doc in enumerate(docs)}\n",
    "df = pd.DataFrame(countt).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df.index\n",
    "ds = df.values.T.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['a', 'aahed', 'abandon', 'abandoned', 'abc', 'abcs', 'abdomen',\n",
       "       'aberdeen', 'abilities', 'ability',\n",
       "       ...\n",
       "       'zero', 'zircon', 'zirconium', 'zone', 'zones', 'zoo', 'zookeepers',\n",
       "       'zorenstein', 'zucaro', 'zwick'],\n",
       "      dtype='object', length=9571)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = {}\n",
    "for i,j in enumerate(words):\n",
    "    word_list[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docss = []\n",
    "V = len(words)\n",
    "for k in range(500):\n",
    "    N = len(docs[k])\n",
    "    doc = np.zeros((N))\n",
    "    for i in range(N):\n",
    "        doc[i] = word_list[docs[k][i]] \n",
    "    docss.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs = []\n",
    "for k in range(500):\n",
    "    N = len(docs[k])\n",
    "    doc = np.zeros((N,V))\n",
    "    for i in range(N):\n",
    "        doc[i][int(docss[k][i])] = 1\n",
    "    final_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.694"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([doc.shape[0] for doc in final_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LDA(10,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "finished E\n",
      "-18454.005438189444\n",
      "alpha:36.907826\n",
      "alpha:13.698596\n",
      "alpha:5.163331\n",
      "alpha:2.031227\n",
      "alpha:0.898812\n",
      "alpha:0.523218\n",
      "alpha:0.437401\n",
      "alpha:0.431016\n",
      "alpha:0.430978\n",
      "alpha:0.430978\n",
      "alpha:0.430978\n",
      "alpha change0.020927\n",
      "beta change0.144682\n",
      "finished M\n",
      "iteration 2\n",
      "finished E\n",
      "-15941.373668747523\n",
      "alpha:36.976019\n",
      "alpha:13.792785\n",
      "alpha:5.269534\n",
      "alpha:2.148308\n",
      "alpha:1.035468\n",
      "alpha:0.694051\n",
      "alpha:0.636131\n",
      "alpha:0.634079\n",
      "alpha:0.634077\n",
      "alpha:0.634077\n",
      "alpha change0.041249\n",
      "beta change0.000524\n",
      "finished M\n",
      "iteration 3\n",
      "finished E\n",
      "-14425.641964686401\n",
      "alpha:37.074268\n",
      "alpha:13.928859\n",
      "alpha:5.424026\n",
      "alpha:2.321231\n",
      "alpha:1.241530\n",
      "alpha:0.948300\n",
      "alpha:0.914703\n",
      "alpha:0.914210\n",
      "alpha:0.914210\n",
      "alpha:0.914210\n",
      "alpha change0.078475\n",
      "beta change0.000109\n",
      "finished M\n",
      "iteration 4\n",
      "finished E\n",
      "-13484.135545734833\n",
      "alpha:37.211807\n",
      "alpha:14.120075\n",
      "alpha:5.643166\n",
      "alpha:2.571222\n",
      "alpha:1.544910\n",
      "alpha:1.311589\n",
      "alpha:1.295202\n",
      "alpha:1.295116\n",
      "alpha:1.295116\n",
      "alpha change0.145089\n",
      "beta change0.000022\n",
      "finished M\n",
      "iteration 5\n",
      "finished E\n",
      "-12882.925505643056\n",
      "alpha:37.399348\n",
      "alpha:14.382147\n",
      "alpha:5.947190\n",
      "alpha:2.925745\n",
      "alpha:1.979730\n",
      "alpha:1.811152\n",
      "alpha:1.804611\n",
      "alpha:1.804601\n",
      "alpha:1.804601\n",
      "alpha change0.259575\n",
      "beta change0.000004\n"
     ]
    }
   ],
   "source": [
    "phi_post,gamma_post,alpha_post,beta_post = model.fit(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('phi.npy', phi_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gamma.npy', gamma_post)\n",
    "np.save('alpha.npy', alpha_post)\n",
    "np.save('beta.npy', beta_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_f = np.load('beta.npy')\n",
    "alpha_f = np.load('alpha.npy')\n",
    "phi_f = np.load('phi.npy')\n",
    "gamma_f = np.load('gamma.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(x0):\n",
    "    x = x0.tolist()\n",
    "    max_values = heapq.nlargest(100, x)\n",
    "    index = [0] * 100\n",
    "    for i in range(100):\n",
    "        index[i] = x.index(max_values[i])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['administration', 'allowed', 'announced', 'arab', 'arms', 'asked', 'assistant', 'authorities', 'average', 'baker', 'band', 'base', 'biggest', 'bill', 'blackowned', 'brought', 'campaign', 'cents', 'chemical', 'close', 'comment', 'committee', 'company', 'condition', 'congress', 'day', 'dollars', 'drilling', 'dukakis', 'earlier', 'face', 'family', 'felt', 'financial', 'free', 'friday', 'full', 'futures', 'global', 'government', 'half', 'hands', 'he', 'holding', 'immigration', 'industrial', 'iraq', 'island', 'lead', 'leading', 'living', 'lot', 'major', 'management', 'measures', 'men', 'monday', 'months', 'moore', 'nation', 'nelson', 'note', 'offered', 'official', 'panama', 'people', 'pressure', 'process', 'ratings', 'reduce', 'reserve', 'san', 'shamir', 'state', 'stock', 'talks', 'testified', 'they', 'three', 'times', 'town', 'union', 'united', 'victims', 'visit', 'wanted', 'worked', 'yearold']\n"
     ]
    }
   ],
   "source": [
    "word0 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[0]):\n",
    "        word0.append(key)\n",
    "for w in word0:\n",
    "    if w in stopwords:\n",
    "        word0.remove(w)\n",
    "print(word0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airlines', 'analysts', 'ap', 'area', 'asked', 'assistance', 'been', 'called', 'center', 'central', 'charged', 'checchi', 'city', 'club', 'committee', 'condition', 'current', 'czechoslovakia', 'days', 'death', 'democrats', 'details', 'documents', 'ec', 'election', 'european', 'federal', 'force', 'gasoline', 'germany', 'gorbachev', 'group', 'health', 'hours', 'improve', 'increases', 'iraq', 'jackson', 'john', 'lead', 'lost', 'mikhail', 'military', 'minister', 'month', 'mother', 'noriega', 'numbers', 'oil', 'percent', 'period', 'place', 'plant', 'pm', 'political', 'power', 'produced', 'programs', 'proposed', 'real', 'republican', 'rights', 'sales', 'sandinista', 'saudi', 'secretary', 'service', 'set', 'soviet', 'spoke', 'statement', 'strong', 'students', 'study', 'sunday', 'the', 'time', 'today', 'view', 'violence', 'week', 'white', 'winds', 'workers']\n"
     ]
    }
   ],
   "source": [
    "word1 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[1]):\n",
    "        word1.append(key)\n",
    "for w in word1:\n",
    "    if w in stopwords:\n",
    "        word1.remove(w)\n",
    "print(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aid', 'anonymity', 'appeared', 'armed', 'army', 'attorney', 'awards', 'based', 'began', 'bill', 'billion', 'born', 'bush', 'call', 'case', 'chairman', 'charges', 'city', 'close', 'conference', 'contact', 'control', 'december', 'director', 'district', 'doc', 'drug', 'east', 'economy', 'effort', 'exchange', 'expected', 'family', 'food', 'four', 'friday', 'goods', 'groups', 'growth', 'high', 'hundreds', 'identified', 'involved', 'iraqi', 'killed', 'long', 'met', 'month', 'national', 'northeastern', 'october', 'official', 'part', 'peace', 'plan', 'police', 'pope', 'prime', 'problem', 'public', 'questions', 'reached', 'rebels', 'reporters', 'she', 'shot', 'soviet', 'summer', 'temperatures', 'that', 'thought', 'thousands', 'times', 'top', 'up', 'war', 'washington', 'weapons', 'west', 'york', 'your']\n"
     ]
    }
   ],
   "source": [
    "word2 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[2]):\n",
    "        word2.append(key)\n",
    "for w in word2:\n",
    "    if w in stopwords:\n",
    "        word2.remove(w)\n",
    "print(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'addition', 'administration', 'agency', 'air', 'american', 'angeles', 'animals', 'august', 'being', 'budget', 'carbon', 'change', 'civil', 'considered', 'contract', 'daughter', 'day', 'death', 'denied', 'department', 'diplomatic', 'doctors', 'economic', 'enforcement', 'environmental', 'feel', 'fire', 'gene', 'george', 'government', 'great', 'gulf', 'have', 'house', 'ii', 'increase', 'issued', 'judge', 'jury', 'kuwait', 'least', 'lost', 'march', 'ministry', 'month', 'mundy', 'news', 'november', 'offered', 'officers', 'ohio', 'order', 'others', 'president', 'price', 'problems', 'program', 'provided', 'rate', 'record', 'school', 'services', 'south', 'soviet', 'speaking', 'store', 'strike', 'summit', 'talks', 'their', 'time', 'told', 'tuesday', 'utilities', 'visit', 'wednesday', 'when']\n"
     ]
    }
   ],
   "source": [
    "word3 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[3]):\n",
    "        word3.append(key)\n",
    "for w in word3:\n",
    "    if w in stopwords:\n",
    "        word3.remove(w)\n",
    "print(word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'and', 'bill', 'black', 'business', 'campaign', 'career', 'cash', 'communist', 'companies', 'congress', 'contra', 'daily', 'david', 'days', 'demand', 'died', 'docno', 'dont', 'drop', 'emergency', 'friend', 'gallon', 'hear', 'hearing', 'held', 'houston', 'international', 'israel', 'just', 'kill', 'kravis', 'late', 'latest', 'leave', 'markets', 'military', 'mission', 'monday', 'money', 'morning', 'navy', 'nazi', 'night', 'north', 'oil', 'only', 'orders', 'pentagon', 'person', 'points', 'police', 'policy', 'primary', 'process', 'rose', 'sale', 'saturday', 'small', 'soviet', 'spokesman', 'stand', 'suit', 'supported', 'telephone', 'them', 'things', 'thursday', 'told', 'turn', 'united', 'vote', 'wedtech', 'week', 'weve', 'white', 'would', 'year']\n"
     ]
    }
   ],
   "source": [
    "word4 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[4]):\n",
    "        word4.append(key)\n",
    "for w in word4:\n",
    "    if w in stopwords:\n",
    "        word4.remove(w)\n",
    "print(word4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'agreement', 'america', 'ap', 'at', 'bank', 'board', 'broke', 'california', 'called', 'car', 'change', 'church', 'claimed', 'court', 'decided', 'defense', 'degrees', 'delegates', 'disease', 'dont', 'duracell', 'earlier', 'eastern', 'fernandez', 'flights', 'found', 'gang', 'good', 'gorbachev', 'gorbachevs', 'greyhound', 'ground', 'guerrillas', 'hard', 'head', 'include', 'increased', 'injured', 'japan', 'jewish', 'korean', 'largest', 'leadership', 'left', 'life', 'long', 'major', 'making', 'manufacturing', 'measure', 'medical', 'miles', 'monday', 'nations', 'night', 'northwest', 'oct', 'operation', 'organization', 'party', 'president', 'prison', 'quayle', 'relations', 'remove', 'resign', 'restrictions', 'roberts', 'role', 'say', 'school', 'security', 'soviets', 'street', 'support', 'today', 'told', 'total', 'trust', 'tuesday', 'university', 'visited', 'wall', 'weather', 'with', 'year']\n"
     ]
    }
   ],
   "source": [
    "word5 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[5]):\n",
    "        word5.append(key)\n",
    "for w in word5:\n",
    "    if w in stopwords:\n",
    "        word5.remove(w)\n",
    "print(word5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ago', 'agreement', 'an', 'armenia', 'arrested', 'assets', 'attack', 'baghdad', 'barry', 'board', 'bush', 'called', 'central', 'closing', 'commission', 'complex', 'countries', 'country', 'day', 'democratic', 'deputy', 'didnt', 'during', 'england', 'exports', 'farm', 'fbi', 'force', 'france', 'friends', 'general', 'gold', 'good', 'health', 'her', 'hospital', 'international', 'jan', 'jim', 'john', 'jones', 'large', 'leader', 'live', 'market', 'mass', 'member', 'moved', 'ms', 'northern', 'occupied', 'office', 'officer', 'operating', 'ortega', 'parents', 'pay', 'point', 'political', 'pollution', 'prices', 'review', 'richard', 'roberts', 'security', 'senate', 'shuttle', 'snow', 'sold', 'started', 'states', 'street', 'survey', 'testimony', 'the', 'to', 'tropical', 'truck', 'vice', 'will', 'years']\n"
     ]
    }
   ],
   "source": [
    "word6 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[6]):\n",
    "        word6.append(key)\n",
    "for w in word6:\n",
    "    if w in stopwords:\n",
    "        word6.remove(w)\n",
    "print(word6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adding', 'ago', 'ahead', 'americans', 'announced', 'believed', 'big', 'billion', 'border', 'boston', 'bushs', 'cent', 'cents', 'charge', 'chief', 'coast', 'company', 'continue', 'countries', 'country', 'created', 'docno', 'dollar', 'dow', 'dukakis', 'early', 'efforts', 'executive', 'fact', 'fair', 'fell', 'filed', 'forced', 'forces', 'foreign', 'gun', 'higher', 'hospital', 'including', 'issue', 'january', 'large', 'late', 'law', 'leaders', 'loss', 'lower', 'man', 'mayor', 'meeting', 'member', 'news', 'offer', 'percent', 'president', 'production', 'release', 'remain', 'robert', 'rose', 'sen', 'settlements', 'society', 'southern', 'space', 'star', 'to', 'trial', 'troops', 'was', 'washington', 'waste', 'wednesday', 'west', 'will', 'work', 'wrote', 'year', 'yearold', 'york']\n"
     ]
    }
   ],
   "source": [
    "word7 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[7]):\n",
    "        word7.append(key)\n",
    "for w in word7:\n",
    "    if w in stopwords:\n",
    "        word7.remove(w)\n",
    "print(word7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['administration', 'aid', 'americans', 'anniversary', 'annual', 'area', 'average', 'batalla', 'bombs', 'capital', 'car', 'caused', 'conditions', 'constitution', 'control', 'council', 'county', 'crime', 'dakota', 'damage', 'decision', 'decline', 'drivers', 'estimated', 'figures', 'get', 'group', 'heat', 'henry', 'homes', 'house', 'illegal', 'including', 'increased', 'industry', 'interview', 'june', 'justice', 'kind', 'labor', 'leader', 'liberace', 'make', 'meeting', 'members', 'mexico', 'monday', 'month', 'nations', 'not', 'other', 'payments', 'percent', 'peres', 'peterson', 'poll', 'president', 'previous', 'production', 'rating', 'reagan', 'red', 'released', 'reports', 'required', 'rise', 'season', 'september', 'shamirs', 'signed', 'smith', 'speech', 'state', 'suit', 'system', 'team', 'there', 'this', 'to', 'united', 'well', 'will', 'working', 'year', 'years']\n"
     ]
    }
   ],
   "source": [
    "word8 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[8]):\n",
    "        word8.append(key)\n",
    "for w in word8:\n",
    "    if w in stopwords:\n",
    "        word8.remove(w)\n",
    "print(word8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'agents', 'air', 'american', 'any', 'ap', 'april', 'been', 'billion', 'cars', 'cash', 'central', 'children', 'company', 'congress', 'corp', 'cuban', 'declined', 'democratic', 'doc', 'drugs', 'dukakis', 'employees', 'ended', 'feet', 'front', 'future', 'good', 'government', 'governor', 'grounds', 'he', 'inflation', 'initiative', 'investigation', 'israel', 'james', 'japanese', 'job', 'lawsuit', 'legal', 'local', 'los', 'low', 'lowest', 'main', 'members', 'miles', 'number', 'office', 'officials', 'opposition', 'people', 'pipeline', 'plan', 'policies', 'press', 'red', 'report', 'reported', 'scientists', 'sen', 'sides', 'south', 'southern', 'spending', 'sunday', 'supplies', 'that', 'this', 'today', 'todays', 'trade', 'wars', 'weeks', 'what', 'william', 'women', 'years', 'york']\n"
     ]
    }
   ],
   "source": [
    "word9 = []\n",
    "for key, value in word_list.items():\n",
    "    if value in find_index(beta_f[9]):\n",
    "        word9.append(key)\n",
    "for w in word9:\n",
    "    if w in stopwords:\n",
    "        word9.remove(w)\n",
    "print(word9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
