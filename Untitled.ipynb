{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a common method of topic modelling. It’s a technique that automatically discover the topics that documents contain. It’s a generative topic probabilistic first proposed by David M.Blei, Andrew Y.Ng, and Michae l. Jordan to find the latent variables(topics) in a text corpus (collection of Documents). \n",
    "\n",
    "Latent Dirichlet Allocation is a three level hierarchical Bayesian model for collections of discrete data and implemented by variantional EM algorithm. Variantional inference is a method that approximates probability densities through optimizations and it’s faster and easier to implement & scale than the traditional method such as MCMC. To LDA, a document is just a collection of topics where each topics has a particular probability of generating specific words. We implement variantional EM algorithm to calculate the optimal parameters of probabilistic topic distributions of documents and probabilistic words distributions over topics by using techniques such as posterior inference, Newton-Raphson method, gradient descent, Hessian matrix, etc. By getthing these optimal parameters, we can assign each documents to different topics by probabilities. \n",
    "\n",
    "In this report, we will first introduce and analyze the LDA model in a mathematical way. And then we will implement the core functions and present the pseudo-codes of the LDA model. We also find ways to optimize our algorithm and at last we will use the simulated data and real world data to test the accuracy of our model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backgrounds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research paper we are using is the paper Latent Dirichlet Allocation written by David M Blei, Andrew Y.Ng and Michael I.Jordan. LDA is a common method of topic modelling. A short example is if I have a document and I don't know it’s belonging to the food topic or the fruit topic. After implementing the LDA model I will know the percentage distribution of topics of that document. LDA is not only restricted to topic modelling in text, actually it can be work on any set that can be represented as binary vectors or non binary vectors. However, the most popular usage of LDA is still the topic modelling in text. In order to give a more clear picture on the content of the model, we will present a more specific example. \n",
    "\n",
    "Suppose we have the following set of the sentences (documents):\n",
    "\n",
    "1.\tThere are elephants, tigers, lions, dogs in the city’s zoo. \n",
    "2.\tEat an apple everyday, doctors go away. \n",
    "3.\tHunters said that there are tigers, bears and other wild animals in the forest so it’s very dangerous to go inside the forest alone. \n",
    "4.\tSusan loves to eat tomatoes, broccoli and apple. \n",
    "5.\tUnlike tigers who like to eat meats, rabbits love to eat carrot. \n",
    "\n",
    "LDA is a method that can discover the topics underlying each documents by self-learning:\n",
    "1 & 3: 100% topic A\n",
    "2 % 4: 100 topic B\n",
    "5: 75% topic A and 25% topic B. \n",
    "\n",
    "Topic A: 40% tigers, 20% elephants, …. \n",
    "Topic B: 30% apple, 20% broccoli,… \n",
    "\n",
    "We will introduce the mathematical inductions of LDA and how LDA performs the discovery in the next section. \n",
    "\n",
    "We need LDA because in the real world, the dataset can be very huge with a lot of documents and information. So we need the tools that can search, organize and understand that vast amount of information. LDA provides us a method to summarize that large collections of information and discover the hidden topics under each documents. This is the most known application of LDA in the real world: topic modellings. We can achieve some basic text minings by using the LDA such as novelty detection, classification, relevance judgments and similarity. The advantage of LDA is that it’s easy to implement, very clear logic and it’s a probabilistic model (soft-clustering) with interpretable topics. The limitations of the LDA is that the method assumes the number of topics in the dataset are specified by the user or based on a poisson distribution, but this is subjective and always did not actually highlight the true distribution of the topics. Second limitation is the relatively long running time than the other natural language processing alrogithms. The thrid limitation is that LDA did not consider the order of the words when it's learning. Sometimes the orders of the word may also affect the topic that document belonging to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Declaration: \n",
    "\n",
    "K: number of topics \n",
    "\n",
    "M: number of documents\n",
    "\n",
    "N: Number of words in the documents \n",
    "\n",
    "$\\theta_m$: distribution for document m\n",
    "\n",
    "$\\alpha$: a K dimensional vector, the topic distribution of K topics. \n",
    "\n",
    "$\\beta$: K x V matrix, $\\beta_{ij}$ means the probability of $j^{th}$ word of $i^{th}$ topic. \n",
    "\n",
    "z: topics \n",
    "\n",
    "w: words \n",
    "\n",
    "D: Collection of Documents \n",
    "\n",
    "$\\phi$: M x N x K matrix, means probability distribution of each topic of words in each documents. \n",
    "\n",
    "$\\gamma$: M x K matrix, means the probability distribution of topics for each documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA assumes that the latent topics of a document comes from a multinomial distribution and the words also comes from a multinomial distribution conditional on the latent topics.\n",
    "\n",
    "In this problem,we have a set of M documents, D, which are mixture of latent topics we have a dictionary of unique words,W, with size V. We donate N as the number of words in a document.\n",
    "\n",
    "The generative process of LDA is described below.\n",
    "\n",
    "- 1. choose N (the number of words in the document) from a poisson distribution with $\\lambda=\\xi$, $N\\sim Poisson(\\xi)$\n",
    "    \n",
    "- 2. choose $\\theta\\sim Dir(\\alpha)$ \n",
    "\n",
    "- for each of N words $w_n$:\n",
    "        \n",
    "- (a) choose a topic $z_n \\sim Multinomial(\\theta)$\n",
    "\n",
    "- (b) choose a word $w_n \\sim Multinomial(\\beta_{z_n})$, $\\beta \\in \\mathbb{R}^{k \\times V}$ and $\\beta_{z_n}$ is the $z_n$ row of the $\\beta$\n",
    "\n",
    "The probability density of Dirichlet distribution is $p(\\theta|\\alpha)=\\frac{\\Gamma(\\sum_{i=1}^k\\alpha_i)}{\\prod_{i=1}^k\\Gamma(\\alpha_i)}\\theta_1^{\\alpha_1-1}...\\theta_k^{\\alpha_k-1}$ where $\\alpha \\in \\mathbb{R}^k$ ,$\\theta \\in \\mathbb{R}^k$ and $\\sum_{i=1}^k\\theta_i=1$, $\\alpha_i\\gt0$\n",
    "\n",
    "The Dirichlet distribution is an expansion of Gamma distribution into multivariable space and it is the conjugate distribution of multinomial distribution which can make our calculation much more easier.\n",
    "\n",
    "From the assumption above, we can easily derive following equations.\n",
    "\n",
    "$p(\\theta,z,w|\\alpha,\\beta)=p(\\theta|\\alpha)\\prod_{n=1}^Np(z_n|\\theta)p(w_n|z_n,\\beta)$\n",
    "\n",
    "\n",
    "$p(\\theta,w|\\alpha,\\beta) = p(\\theta|\\alpha)\\prod_{n=1}^N(\\sum_{z_n}p(z_n|\\theta)p(w_n|z_n,\\beta))$\n",
    "\n",
    "The likelihood of a single document can be expressed as\n",
    "\n",
    "$$p(w|\\alpha,\\beta)=\\int p(\\theta|\\alpha)\\prod_{n=1}^N(\\sum_{z_n}p(z_n|\\theta)p(w_n|z_n,\\beta))d\\theta$$\n",
    "\n",
    "And the likelihood of Corpus is:\n",
    "\n",
    "$$\n",
    "p(D|\\alpha,\\beta)=\\prod_{d=1}^M\\int p(\\theta_d|\\alpha)\\prod_{n=1}^{N_d}(\\sum_{z_n}p(z_n|\\theta_d)p(w_n|z_n,\\beta))d\\theta_d\n",
    "$$\n",
    "\n",
    "The problem is transformed to finding the \n",
    "\n",
    "$$\n",
    "\\alpha^*,\\beta^* = argmax_{\\alpha^*,\\beta^*}p(D|\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "Since there is no closed form solution, we can use the variant EM algrithm below to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution of hidden variables that we have to compute is:\n",
    "\n",
    "$p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta) = \\frac{p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)}{p(\\textbf{w}|\\alpha,\\beta)}$\n",
    "\n",
    "Unfortunately, this distribution is intractable to compute in general. \n",
    "\n",
    "So indeed we have to use the variaitonal distribution on the latent variables which is:\n",
    "\n",
    "$q(\\theta,\\textbf{z}|\\gamma,\\phi) = q(\\theta|\\gamma)\\prod q(z_n|\\phi_n)$ where the Dirichlet parameter $\\gamma$ and the multinomial parameter $(\\phi_1,...,\\phi_n)$ are the free variational parameters. \n",
    "\n",
    "Then the next step will be set up an optimization problem that to determine the values of variational parameters $\\gamma$ and $\\phi$:\n",
    "\n",
    "$(\\gamma^*,\\phi^*) = arg min_{(\\gamma,\\phi)}D(q(\\theta,\\textbf{z}|\\gamma,\\phi)||p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta))$\n",
    "\n",
    "Then we use the method of the KL divergence:\n",
    "\n",
    "the KL divergence between $q(\\theta,\\textbf{z}|\\gamma,\\phi)$ and $p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta)$ is \n",
    "\n",
    "$E[log q(\\theta,\\textbf{z}|\\gamma,\\phi)] - E[logp(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta)]$\n",
    "\n",
    "= $E[log q(\\theta,\\textbf{z}|\\gamma,\\phi)] - E[logp(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)] + log p(\\textbf{w}|\\alpha,\\beta)$\n",
    "\n",
    "By applying the Jensen's equality:\n",
    "\n",
    "$log p(\\textbf{w}|\\alpha,\\beta) = log \\int \\sum p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)d\\theta$\n",
    "\n",
    "= $log \\int \\sum \\frac{p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)q(\\theta,\\textbf{z})}{q(\\theta,\\textbf{z})d\\theta}$\n",
    "\n",
    "$\\geq \\int \\sum q(\\theta,\\textbf{z})log p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)d\\theta - \\int\\sum q(\\theta,\\textbf{z}) log q(\\theta,\\textbf{z})d\\theta$\n",
    "\n",
    "= $E[log p(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)] - E[q(\\theta,\\textbf{z})]$\n",
    "\n",
    "Let $L(\\gamma,\\phi;\\alpha,\\beta) = E[logp(\\theta,\\textbf{z},\\textbf{w}|\\alpha,\\beta)] - E[log q(\\theta,\\textbf{z}]$ then we have:\n",
    "\n",
    "$log p(\\textbf{w}|\\alpha,\\beta) = L(\\gamma,\\phi;\\alpha,\\beta) + D(q(\\theta,\\textbf{z}|\\gamma,\\phi)||p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta))$ \n",
    "\n",
    "So maximizing the lower bound $L(\\gamma,\\phi;\\alpha,\\beta)$ is equivalent to minimizing the KL divergence between between the variational posterior probabilty and the true posterior probability. \n",
    "\n",
    "$L(\\gamma,\\phi;\\alpha,\\beta) = E[log p(\\theta|\\alpha)] + E[log p(\\textbf{z}|\\theta)] + E[log p(\\textbf{w}|\\textbf{z},\\beta)] - E[log q(\\theta) - E[log q(z)]]$\n",
    "\n",
    "We set the derivative of L and set them equal to 0, we obtain the updated equation of $\\phi$ and $\\gamma$:\n",
    "\n",
    "$\\phi_{ni} \\propto \\beta_{iv}exp{\\psi(\\gamma_i)-\\psi(\\sum \\gamma_j)}$\n",
    "\n",
    "$\\gamma_i = \\alpha_i + \\sum \\phi_{ni}$ \n",
    "\n",
    "where $\\psi$ is the digamma function, which is the first derivative of the log gamma. \n",
    "\n",
    "\n",
    "So the pseudocode is:\n",
    "\n",
    "initialize $\\phi_{ni}^0 = \\frac{1}{K}$ for all i and n. \n",
    "\n",
    "initialize $\\gamma_i^0 = \\alpha_i + \\frac{N}{K}$ for all i. \n",
    "\n",
    "repeat:\n",
    "\n",
    "for n = 1 to N:\n",
    "\n",
    "for i = 1 to K:\n",
    "\n",
    "$\\phi_{ni}^{t+1} = \\beta_{iw_{n}}exp{\\psi(\\gamma_i^t)}$.\n",
    "\n",
    "normalize $\\phi_{ni}^{t+1}$ to make their sums to 1.\n",
    "\n",
    "$\\gamma^{t+1} = \\alpha + \\sum \\phi_{ni}^{t+1}$. \n",
    "\n",
    "until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(doc,alpha,beta,phi0,gamma0,max_iter=500,tol=1e-3):\n",
    "    N = doc.shape[0]\n",
    "    V = doc.shape[1]\n",
    "    topic_num = len(alpha)\n",
    "    \n",
    "    phi,gamma = phi0,gamma0\n",
    "    tmp_phi,tmp_gamma = phi0,gamma0\n",
    "    delta_phi,delta_gamma = 9999,9999\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        for n in range(N):\n",
    "            for j in range(topic_num):\n",
    "                phi[n, j] = np.sum(beta[j,]*doc[n,]) * np.exp(digamma(gamma[j]))\n",
    "            phi[n,] = phi[n,] / np.sum(phi[n,])\n",
    "            gamma = alpha + np.sum(phi[n,])\n",
    "            delta_phi = np.sum((phi - tmp_phi) ** 2)\n",
    "            delta_gamma = np.sum((gamma - tmp_gamma) ** 2)\n",
    "            tmp_phi,tmp_gamma = phi, gamma\n",
    "            if((delta_phi <= tol) and (delta_gamma <= tol)):\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "            break \n",
    "    return phi, gamma \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M step\n",
    "M step is finding the MLE(maximum likelihood estimates) with expected sufficient statistics.\n",
    "\n",
    "\n",
    "Since the $\\alpha$, $\\beta$ is isolated with expectated sufficient statistics, we can isolate the log likelihood function with two term.\n",
    "\n",
    "\n",
    "First, we find the new $\\alpha$.\n",
    "\n",
    "$L_{\\alpha} = \\sum_{d=1}^M(log(\\sum_{j=1}^k\\alpha_i)-\\sum_{i=1}^klog\\alpha_i+\\sum_{i=1}^k(\\alpha_i-1)(\\Phi(\\gamma_di)-\\Phi(\\sum_{j=1}^k\\gamma_dj))))$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial\\alpha_i}=M(\\Phi(\\sum_{j=1}^k\\alpha_j)-\\Phi(\\alpha_i))+\\sum_{d=1}^M(\\Phi(\\gamma_{di}-\\Phi(\\sum_{j=1}^k\\gamma_{dj})))$\n",
    "\n",
    "The derivative depends on $\\alpha$, and therefore we need to derive a iterative way to find the optimal solution.\n",
    "\n",
    "Further we can get\n",
    "\n",
    "$\\frac{\\partial L}{\\partial\\alpha_i\\alpha_j} = M\\Psi'(\\sum_{j=1}^k\\alpha_j)-\\delta(i,j)M\\Psi'(\\alpha_i)$\n",
    "Newton-Raphson algorithm to find the optimal solution.\n",
    "\n",
    "Since the Hessian matrix can be written as $H = diag(-M\\Psi'(\\alpha))+1(M\\Psi'(\\sum_{j=1}^k\\alpha_j))1^T$\n",
    "\n",
    "To generilize it, $H=diag(h)+1z1^T$ and applying the matrix inverse lemma we can get $H^{-1}=dig(h)^{-1}-\\frac{diag(h)^{-1}11^Tdiag(h)^{-1}}{z^{-1}+\\sum_{j=1}^kh_j^{-1}}$ \n",
    "\n",
    "Therefore $(H^{-1}g)_i=\\frac{g_i-c}{h_i}$ where $c=\\frac{\\sum_{j=1}^kg_j/h_j}{z^{-1}+\\sum_{j=1}^kh_j^{-1}}$\n",
    "\n",
    "and we can use the update formula $\\alpha^{t+1} = \\alpha^{t}-H^{-1}g$ to get the result in a linear time complexity.\n",
    "\n",
    "\n",
    "Then, we find the new $\\beta$\n",
    "\n",
    "Since the $\\beta$ is constrained that $\\sum_{j=1}^V\\beta_ij=1$ for any $i$\n",
    "\n",
    "By adding Lagrange Multipliers, we get\n",
    "\n",
    "$L_{[\\beta]}=\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\sum_{i=1}^k\\sum_{j=1}^V \\psi_{dni}W^j_{dn}log\\beta_{ij}+\\sum_{i=1}^k\\lambda_i(\\sum_{j=1}^V\\beta_{ij}-1)$\n",
    "\n",
    "Taking the derivative with repect to $\\beta_{ij}$, we get\n",
    "\n",
    "$$\\frac{\\partial L_{[\\beta]}}{\\partial\\beta_{ij}} =\\sum_{d=1}^M\\sum_{n=1}^{N_d} \\psi_{dni}W^j_{dn}\\frac{1}{\\beta_{ij}}+\\lambda_i$$\n",
    "\n",
    "Setting it to zeros,\n",
    "$$\\beta_{ij}=-\\frac{1}{\\lambda_i}\\sum_{d=1}^M\\sum_{n=1}^{N_d} \\psi_{dni}W^j_{dn}$$\n",
    "\n",
    "Therefore,\n",
    "$$\\beta_{ij} = \\frac{\\sum_{d=1}^M\\sum_{n=1}^{N_d} \\psi_{dni}W^j_{dn}}{\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\sum_{j=1}^V \\psi_{dni}W^j_{dn}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### M step pseudocode\n",
    "\n",
    "1. updating $\\alpha$\n",
    "\n",
    "using linear time Newton-Raphson algorithm to find optimal $\\alpha$\n",
    "\n",
    "repeat:\n",
    "\n",
    "for i=1 to k\n",
    "\n",
    "$g_i = M(\\Psi(\\sum_{j=1}^k\\alpha^t_j)-\\Psi(\\alpha^t_i))+\\sum_{d=1}^M(\\Psi(\\gamma_{di}-\\Psi(\\sum_{j=1}^k\\gamma_{dj})))$\n",
    "\n",
    "$h_i = -M\\Psi'(\\alpha^t_i)$\n",
    "\n",
    "$c=\\frac{\\sum_{j=1}^kg_j/h_j}{z^{-1}+\\sum_{j=1}^kh_j^{-1}}$\n",
    "\n",
    "$\\alpha_i^{t+1}=\\alpha_i^{t}-\\frac{g_i-c}{h_i}$\n",
    "\n",
    "until converge\n",
    "\n",
    "2. updating $\\beta$\n",
    "\n",
    "for i=1 to k:\n",
    "\n",
    "for j=1 to V:\n",
    "$$\\beta_{ij} = \\frac{\\sum_{d=1}^M\\sum_{n=1}^{N_d} \\phi_{dni}W^j_{dn}}{\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\sum_{j=1}^V \\phi_{dni}W^j_{dn}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import polygamma\n",
    "from scipy.special import gammaln\n",
    "from scipy.special import digamma\n",
    "import numpy as np\n",
    "\n",
    "MAX_ALPHA_ITER = 100\n",
    "NEWTON_THRESH = 1e-10\n",
    "\n",
    "def _ss(gamma):\n",
    "    return digamma(gamma)-digamma(gamma.sum(1))[:,np.newaxis]\n",
    "\n",
    "def alhood(a, ss, M, k):\n",
    "    return M*(gammaln(np.sum(a))-np.sum(gammaln(a)))+np.sum(ss%*%a)\n",
    "\n",
    "def d_alhood(a, ss, M, k):\n",
    "    return M*(digamma(np.sum(a))-(digamma(a))) + np.sum(ss,axis=0)\n",
    "\n",
    "def d2_alhood(a, M):\n",
    "    return -M*polygamma(1,a)\n",
    "\n",
    "def optimal_a(ss, M, k):\n",
    "    a = np.ones(k)\n",
    "    for(i in range(MAX_ALPHA_ITER)):\n",
    "        \n",
    "        df = d_alhood(a, ss, M, k)\n",
    "        if np.sum(df**2)<NEWTON_THRESH:\n",
    "            break\n",
    "        d2f = d2_alhood(a, M)\n",
    "        z = M*digamma(np.sum(a))\n",
    "        c = np.sum(df/d2f)/(1/z+np.sum(1/d2f))\n",
    "        a -= (df-c)/d2f\n",
    "    \n",
    "    return a\n",
    "    \n",
    "        \n",
    "\n",
    "def M_step(a,b,phi,gamma, W, M, k, V):\n",
    "    \"\"\"\n",
    "    alpha: k*1\n",
    "    beta: k*V\n",
    "    phi: M*N*k\n",
    "    gamma: M*k\n",
    "    W: M*Nd*V\n",
    "    \n",
    "    M: number of documents\n",
    "    k: number of topic\n",
    "    \"\"\"\n",
    "    ##update alpha\n",
    "    ss = _ss(gamma)\n",
    "    alpha = optimal(ss, M, k)\n",
    "    \n",
    "    ##update beta\n",
    "    beta = mp.zeros((k,V))\n",
    "    for i in range(k):\n",
    "        temp = np.array([np.sum(phi[:,:,i]*W[:,:,j]) for j in range(V)])\n",
    "        beta[i,:] = temp/np.sum(temp)\n",
    "    \n",
    "    return alpha, beta\n",
    "            \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
